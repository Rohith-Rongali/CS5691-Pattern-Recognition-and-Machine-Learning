# -*- coding: utf-8 -*-
"""assign4_sd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B4UGYS4zngcJ8B6Mfb4QlcJu9W3ls9Ud

Implement K-NN and logistic regression on the given data.
Also apply PCA, LDA to the given input vectors and compare the two methods.
"""



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
from sklearn.metrics import det_curve
from mpl_toolkits import mplot3d
from sklearn.metrics import DetCurveDisplay
import scipy.stats as scs
import statistics
from statistics import mode
from collections import Counter

def normalise_scores(S):
  return S/np.sum(S,axis=1)[:,np.newaxis]

def ROC(S,groundtruth):

  thresh = np.linspace(np.amin(S),np.amax(S),200)

  TPR=[]
  FPR=[]
  FNR=[]
  for t in thresh:
    tp=0
    fp=0
    tn=0
    fn=0

    for i in range(len(groundtruth)):
      for j in range(2):
        if S[i][j]>=t:
          if groundtruth[i] == j+1:
            tp+=1
          else:
            fp+=1
        else:
          if groundtruth[i] == j+1:
            fn+=1
          else:
            tn+=1

    TPR.append(tp/(tp+fn))
    FPR.append(fp/(fp+tn))
    FNR.append(fn/(tp+fn))

  return FPR,TPR,FNR

"""Getting the data"""

train_data = 'Synthetic_Data/21/train.txt'
file1= open(train_data)
train1=np.array([[float(lines.split(',')[0]),float(lines.split(',')[1]),int(lines.split(',')[2])] for lines in file1.readlines()],dtype=np.float64)
train_tb = pd.DataFrame(train1,columns=['x','y','class'])

#Normalising the data
train_norm = pd.DataFrame((train_tb[['x','y']].values - np.mean(train_tb[['x','y']].values, axis=0)) / np.std(train_tb[['x','y']].values, axis=0),columns=['x','y'])
train_norm['class'] = train_tb[['class']].values

dev_data = 'Synthetic_Data/21/dev.txt'
file2= open(dev_data)
dev1=np.array([[float(lines.split(',')[0]),float(lines.split(',')[1]),int(lines.split(',')[2])] for lines in file2.readlines()],dtype=np.float64)
dev_tb = pd.DataFrame(dev1,columns=['x','y','class'])

dev_norm = pd.DataFrame((dev_tb[['x','y']].values - np.mean(train_tb[['x','y']].values, axis=0)) / np.std(train_tb[['x','y']].values, axis=0),columns=['x','y'])
dev_norm['class'] = dev_tb[['class']].values


class1 = train_norm.loc[train_tb['class']==1]
class2 = train_norm.loc[train_tb['class']==2]

#train_tb.values[:,:-1]
#train_tb['class'].values

"""PCA"""

cov_matrix = np.cov(train_norm[['x','y']],rowvar = False)
eigval,eigvec = np.linalg.eig(cov_matrix)
order = np.absolute(eigval).argsort()[::-1]
eigval = eigval[order]
eigvec = eigvec[:,order]

plt.figure()
plt.scatter(class2[['x','y']].values[:,0], class2[['x','y']].values[:,1], marker='o',facecolors='none', edgecolors='g', label='Class 2')
plt.scatter(class1[['x','y']].values[:,0], class1[['x','y']].values[:,1], marker='^',facecolors='none', edgecolors='r', label='Class 1')
plt.quiver(0,0,eigvec[0][0],eigvec[0][1],scale=3*eigval[0])
plt.quiver(0,0,eigvec[1][0],eigvec[1][1],scale=3*eigval[1])

"""LDA"""

class1 = train_norm.loc[train_tb['class']==1]
class2 = train_norm.loc[train_tb['class']==2]
S_w = np.cov(class1.values[:,:-1],rowvar = False)+np.cov(class2.values[:,:-1],rowvar = False)
m1 = np.mean(class1.values[:,:-1],axis=0)
m2 = np.mean(class2.values[:,:-1],axis=0)
w_lda = np.linalg.pinv(S_w)@(m1-m2)
plt.figure()
plt.scatter(class2[['x','y']].values[:,0], class2[['x','y']].values[:,1], marker='o',facecolors='none', edgecolors='g', label='Class 2')
plt.scatter(class1[['x','y']].values[:,0], class1[['x','y']].values[:,1], marker='^',facecolors='none', edgecolors='r', label='Class 1')
plt.quiver(0,0,w_lda[0],w_lda[1],scale=1)

def knn_pt(pt,train,labels,k): #takes a point assigns a class
  order = np.argsort(np.sum((train-pt)**2,axis=1))
  c = Counter(labels[order][:k])
  return c     #c.most_common(1)[0][0]

def knn(k,train_norm,train_label,test_norm):
  scores = []
  for x in test_norm:
    temp =[]
    c = knn_pt(x,train_norm,train_label,k)
    for i in range(2):
      temp.append(c[i+1])
    scores.append(temp)
  return scores

#Performing k-NN on synthetic data
k=10
scores1 = knn(10,train_tb.values[:,:-1],train_tb['class'].values,dev_tb.values[:,:-1])

pred_label = np.argmax(scores1,axis=1)+1
confusion_matrix(dev_tb['class'],pred_label)

X = train_tb.iloc[:, 0:2].values
y = train_tb.iloc[:, 2].values
test_norm = dev_tb.values[:, 0:2]
dev_label = dev_tb['class'].values

scores = []
for k in [5,10,15,20]:
  scores.append(knn(k,X,y,test_norm))
  cft = confusion_matrix(np.argmax(scores[-1],axis=1)+1,dev_label)
  print(np.trace(cft)/np.sum(cft))
  print(cft)
m1,m2,m3,m4 = ROC(normalise_scores(scores[0]),dev_label),ROC(normalise_scores(scores[1]),dev_label),ROC(normalise_scores(scores[2]),dev_label),ROC(normalise_scores(scores[3]),dev_label)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='k=5')
plt.plot(m2[0],m2[1],color='g',label='k=10')
plt.plot(m3[0],m3[1],color='b',label='k=15')
plt.plot(m4[0],m4[1],color='y',label='k=20')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")


fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='model-1').plot(ax)
DetCurveDisplay(fpr=m2[0],fnr=m2[2],estimator_name='model-2').plot(ax)
DetCurveDisplay(fpr=m3[0],fnr=m3[2],estimator_name='model-3').plot(ax)
DetCurveDisplay(fpr=m4[0],fnr=m4[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")





print("-----------------------------------------------------------")
print("starting logisitic regression")

def softmax(x):
  #denom = np.sum(np.exp(x),axis=1)
  #return np.exp(x)/denom[:, np.newaxis]
  return (np.exp(x).T / np.sum(np.exp(x),axis=1)).T


def one_hot_enc(Y,nc):
  return np.array([np.eye(1,nc,int(y-1))[0] for y in Y])

def gradient_descent(X,Y,weight,lr):
  nc = Y.shape[1]
  Y_p = softmax(np.matmul(X,weight))

  N = X.shape[0]

  return (1/N)*np.dot(X.T, (Y_p-Y))

def loss(X,Y,weight): 
  N = X.shape[0]
  return -(np.sum(Y * np.log(softmax(np.matmul(X,weight)))))/N

def logistic_reg(X,Y,lr=0.1,iters=500):
    nf = X.shape[1]
    nc = Y.shape[1]
    N = Y.shape[0]

    loss_log =[]
    epoch = 1

    weight = np.ones((nf, nc),dtype=np.float64)

    while(epoch<iters):

      gradients = gradient_descent(X,Y,weight,lr)
      weight = weight- lr*gradients
      loss_log.append(loss(X,Y,weight))
      
      if (epoch%10 ==0):
        print('iter :'+str(epoch))
        print('Loss :'+str(loss_log[-1]))
        #print(grad)
        print('-----------')

      epoch+=1

    return weight

def predict(X,weight):
  return np.argmax(softmax(X@weight),axis=1)+1

def poly_2d(arr,degree):
  pol=[]
  for i in range(degree+1):
    for j in range(i+1):
      pol.append((arr[0]**(i-j))*(arr[1]**j))
  return pol

x_d = np.amax(train_tb['x'].values)-np.amin(train_tb['x'].values)
y_d = np.amax(train_tb['y'].values)-np.amin(train_tb['y'].values)



train_norm = train_tb[['x','y']].values/np.array([x_d,y_d])
test_norm = dev_tb[['x','y']].values/np.array([x_d,y_d])





train_poly = np.array([poly_2d(x,80) for x in train_norm])
dev_poly = np.array([poly_2d(x,80) for x in test_norm])

weight = logistic_reg(train_poly,one_hot_enc(train_tb['class'].values,2))
pred = predict(dev_poly,weight)

cft = confusion_matrix(pred,dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)



weight = logistic_reg(train_tb[['x','y']].values,one_hot_enc(train_tb['class'].values,2))
pred = predict(dev_tb[['x','y']].values,weight)

cft = confusion_matrix(pred,dev_tb['class'].values)
print(np.trace(cft)/np.sum(cft))
print(cft)










print("-----------------------------------------------------------")
print("starting SVM")


"""# SVM"""



from sklearn import svm
svc = svm.SVC()
from sklearn.model_selection import GridSearchCV

#create a classifier
cls = svm.SVC(gamma=0.1, kernel="rbf", probability = True)
X = train_tb.iloc[:, 0:2].values
y = train_tb.iloc[:, 2].values

#train the model
cls.fit(X,y)
#predict the response
pred = cls.predict_proba(dev_tb.values[:, 0:2])

cft = confusion_matrix(np.argmax(pred, axis = 1)+1, dev_tb['class'].values)
#print(np.trace(cft)/np.sum(cft))
#print(cft)
print(cft)

scores = pred

m1 = ROC(normalise_scores(scores),dev_tb['class'].values)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='ANN')

#plt.plot(m4[0],m4[1],color='k',label='k=200')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")


fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='model-1').plot(ax)
#DetCurveDisplay(fpr=m5[0],fnr=m5[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")





print("-----------------------------------------------------------")
print("starting ANN")

"""#ANN

"""

from keras.models import Sequential
from keras.layers import Dense

from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

X = train_tb.iloc[:, 0:2].values
y = train_tb.iloc[:, 2].values

label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(y)
print(integer_encoded)

onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

onehot_encoded


classifier = Sequential()
classifier.add(Dense(units = 32, activation = 'relu', input_dim = 2))
classifier.add(Dense(units = 16, activation = 'relu'))
classifier.add(Dense(units = 4, activation = 'relu'))
classifier.add(Dense(units = 2, activation = 'softmax'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier.fit(X, onehot_encoded, batch_size = 10, epochs = 90)

y_pred = classifier.predict(dev_tb.values[:, 0:2])

dev_res = dev_tb.values[:, 2]
dev_res = np.array(dev_res)

cft = confusion_matrix(np.argmax(y_pred, axis = 1)+1, dev_res)

print(cft)

print("-----------------------------------------------------------")
print("DONE")

plt.show()


