# -*- coding: utf-8 -*-
"""assign4_id.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nnVRyqSk7VajNVpCdDk1YBiHST45nq-A
"""


import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
from sklearn.metrics import det_curve
from mpl_toolkits import mplot3d
from sklearn.metrics import DetCurveDisplay
import scipy.stats as scs
import statistics
from statistics import mode
from collections import Counter

from sklearn import svm
svc = svm.SVC()
from sklearn.model_selection import GridSearchCV

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense

from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

"""#KNN

Getting the data
"""

def normalise_scores(S):
  return S/np.sum(S,axis=1)[:,np.newaxis]

def ROC(S,groundtruth):

  thresh = np.linspace(np.amin(S),np.amax(S),200)
  #thresh = np.sort(list(set(np.ravel(scores))))
  TPR=[]
  FPR=[]
  FNR=[]
  for t in thresh:
    tp=0
    fp=0
    tn=0
    fn=0

    for i in range(len(groundtruth)):
      for j in range(5):
        if S[i][j]>=t:
          if groundtruth[i] == j:
            tp+=1
          else:
            fp+=1
        else:
          if groundtruth[i] == j:
            fn+=1
          else:
            tn+=1

    TPR.append(tp/(tp+fn))
    FPR.append(fp/(fp+tn))
    FNR.append(fn/(tp+fn))

  return FPR,TPR,FNR

imgs = ['coast','forest','highway','mountain','opencountry']

def get_train(dir):
    file_iter=0
    train = []
    for filename in os.listdir(dir):
        if filename != '.ipynb_checkpoints':
            f = open(dir+'/'+filename)
            lines_list = f.readlines()
            temp = np.array([ np.array(lines_list[i].split(),dtype=np.float64)   for i in range(len(lines_list))  ],dtype=np.float64)
            train.append(temp)
     
    return train

train = []
for i in range(5):
    dir  = 'Features/'+imgs[i]+'/train'
    train.append(get_train(dir))

dev =[]
dev_label = []
for t in range(5):
  temp = get_train('Features/'+imgs[t]+'/dev')
  dev_label= dev_label +[t for i in range(len(temp))]
  dev = dev + temp




images = []    #1210 training examples each vector of size 828 
image_label = []
a = np.array(train[0]+train[1]+train[2]+train[3]+train[4])
b = a.reshape((1210*36,23))
ms = (np.amin(b,axis=0),np.amax(b,axis=0))
diff = ms[1] - ms[0]
for k in range(len(train)):
    for i in range(len(train[k])):
            images.append(np.ravel(100*(train[k][i]-ms[0])/diff))
            image_label.append(k) 
images = np.array(images)
image_label = np.array(image_label)

test = []    
for i in range(len(dev)):
 test.append(np.ravel(100*(dev[i]-ms[0])/diff))
test = np.array(test)



#np.array(train[0]
a = np.array(train[0]+train[1]+train[2]+train[3]+train[4])
b = a.reshape((1210*36,23))
ms = (np.amin(b,axis=0),np.amax(b,axis=0))
diff = ms[1] - ms[0]
100*(train[0][0]-ms[0])/diff

x=np.array([[1,2,3],[3,4,5]])
y = np.array([4,5,9])
x-y

"""Normalise the data"""

train_norm = (images - np.mean(images, axis=0)) / np.std(images, axis=0)
test_norm = (test- np.mean(images, axis=0)) / np.std(images, axis=0)

""" PCA and LDA """


def empirical_covariance(X):
  s = X.shape[1]
  cov  = np.zeros((s,s),dtype = np.float64)
  for t in X:
    cov += np.outer(t,t)
  cov = cov/len(X) - np.outer(np.mean(X,axis=0),np.mean(X,axis=0))
  return cov

import scipy
def LDA(X,T,nClasses):
    X = np.array(X,dtype = np.float64)

    
    d = len(X[0])
    # overall mean
    m = np.mean(X,axis = 0)
    
    # categorizing data
    class_X = [[] for i in range(nClasses)]
    for i in range(len(X)):
        class_X[T[i]].append(X[i])
    
    class_mean = [np.mean(class_X[i],axis = 0) for i in range(nClasses)]
    N = [len(class_X[i]) for i in range(nClasses)]
    


    _, y_t = np.unique(T, return_inverse=True)  # non-negative ints
    priors = np.bincount(y_t) / float(len(T))

    classes = np.unique(T)
    SW = np.zeros((d,d))
    for idx, group in enumerate(classes):
        Xg = X[T == group, :]
        SW += priors[idx] * np.atleast_2d(empirical_covariance(Xg))
        # print(priors[idx])
    
    
    ST = empirical_covariance(X)
    SB = ST - SW

    
    if(is_pos_def(SW)):
        eig_values,eig_vecs = scipy.linalg.eigh(SB,SW)
    else:
        eig_values,eig_vecs = np.linalg.eigh(np.linalg.pinv(SW) @ (SB))
    
    
    
    sorted_indices = np.argsort((eig_values))[::-1]


    eig_values = eig_values[sorted_indices]
    eig_vecs = eig_vecs[:,sorted_indices]
    

    return eig_vecs[:,:nClasses-1]


def is_pos_def(x):
    return np.all(np.linalg.eigvals(x) > 0)

ldac = LDA(images,image_label,5)



"""PCA"""

cov_matrix2 = np.cov(train_norm,rowvar = False)
eigval,eigvec = np.linalg.eigh(cov_matrix2)
order = np.absolute(eigval).argsort()[::-1]
eigval = eigval[order]
eigvec = eigvec[:,order]

plt.figure(figsize=(9,6))
plt.plot(np.absolute(eigval))
plt.title(" Plot of eigenvalues magnitude(descending order) in log-scale",size='x-large')
plt.yscale('log')
plt.xlabel("k")
plt.ylabel("eigenvalues magnitude")
plt.grid()

PC = [eigvec[:,:25],eigvec[:,:50],eigvec[:,:100],eigvec[:,:150],eigvec[:,:200]]

"""LDA"""





def LDA(X,y,k,plot =False): 
    nf = X.shape[1]
    n= X.shape[0]
    class_labels = np.unique(y)

    S_w = np.zeros((nf, nf),dtype=np.float64)


    S_t =  np.cov(X.T,dtype=np.float64)
        
    for c in class_labels:
      class_items = np.flatnonzero(y == c)
      S_w = S_w + np.cov(X[class_items].T)# * (len(class_items)-1)
        
    S_b = S_t - S_w
    _, eigvec = np.linalg.eigh(np.linalg.pinv(S_w).dot(S_b))
    ldac = eigvec[:,::-1][:,:k]

    tx = X.dot(ldac)

    colors = ['r','g','b','y','k']
    labels = np.unique(y)
    for color, label in zip(colors, labels):
      class_data = tx[np.flatnonzero(y==label)]
      #plt.scatter(class_data[:,0],class_data[:,1],c=color)
    plt.show()

    return ldac


ldac = LDA(train_norm,image_label,4)

"""Functions for K-NN and logistic regression"""

def knn_pt(pt,train,labels,k): #takes a point assigns a class
  order = np.argsort(np.sum((train-pt)**2,axis=1))
  c = Counter(labels[order][:k])
  return c     #c.most_common(1)[0][0]

def knn(k,train_norm,train_label,test_norm):
  scores = []
  for x in test_norm:
    temp =[]
    c = knn_pt(x,train_norm,train_label,k)
    for i in range(5):
      temp.append(c[i])
    scores.append(temp)
  return scores



def softmax(x):
  #denom = np.sum(np.exp(x),axis=1)
  #return np.exp(x)/denom[:, np.newaxis]
  return (np.exp(x).T / np.sum(np.exp(x),axis=1)).T


def one_hot_enc(Y,nc):
  return np.array([np.eye(1,nc,int(y))[0] for y in Y])

def gradient_descent(X,Y,weight,lr):
  nc = Y.shape[1]
  Y_p = softmax(np.matmul(X,weight))

  N = X.shape[0]

  return (1/N)*np.dot(X.T, (Y_p-Y))

def loss(X,Y,weight): 
  N = X.shape[0]
  return -(np.sum(Y * np.log(softmax(np.matmul(X,weight)))))/N

def logistic_reg(X,Y,lr=0.001,iters=500):
    nf = X.shape[1]
    nc = Y.shape[1]
    N = Y.shape[0]

    loss_log =[]
    epoch = 1

    weight = np.ones((nf, nc),dtype=np.float64)

    while(epoch<iters):

      gradients = gradient_descent(X,Y,weight,lr)
      weight = weight- lr*gradients
      loss_log.append(loss(X,Y,weight))
      
      if (epoch%10 ==0):
        print('iter :'+str(epoch))
        print('Loss :'+str(loss_log[-1]))
        #print(grad)
        print('-----------')

      epoch+=1

    return weight

def predict(X,weight):
  return np.argmax(softmax(X@weight),axis=1),softmax(X@weight)

weight = logistic_reg(train_norm@PC[3],one_hot_enc(image_label,5))
pred = predict(test_norm@PC[3],weight)

cft = confusion_matrix(pred[0],dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

weight = logistic_reg(train_norm,one_hot_enc(image_label,5))
pred1 = predict(test_norm,weight)

cft = confusion_matrix(pred1[0],dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

"""Lets try K-NN"""

scores = []
for k in [5,10,15,20]:
  scores.append(knn(k,train_norm,image_label,test_norm))
  cft = confusion_matrix(np.argmax(scores[-1],axis=1),dev_label)
  print(np.trace(cft)/np.sum(cft))
m1,m2,m3,m4 = ROC(normalise_scores(scores[0]),dev_label),ROC(normalise_scores(scores[1]),dev_label),ROC(normalise_scores(scores[2]),dev_label),ROC(normalise_scores(scores[3]),dev_label)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='k=5')
plt.plot(m2[0],m2[1],color='g',label='k=10')
plt.plot(m3[0],m3[1],color='b',label='k=15')
plt.plot(m4[0],m4[1],color='y',label='k=20')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")

fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='Normal').plot(ax)
DetCurveDisplay(fpr=m2[0],fnr=m2[2],estimator_name='PCA').plot(ax)
DetCurveDisplay(fpr=m3[0],fnr=m3[2],estimator_name='LDA').plot(ax)
#DetCurveDisplay(fpr=m4[0],fnr=m4[2],estimator_name='model-4').plot(ax)
#DetCurveDisplay(fpr=m5[0],fnr=m5[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")

scores1 = knn(15,train_norm ,image_label,test_norm )  # for 40 principal components
cft = confusion_matrix(np.argmax(scores1,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

scores2 = knn(10,train_norm @ eigvec[:,:40],image_label,test_norm @ eigvec[:,:40])  # for 40 principal components
cft = confusion_matrix(np.argmax(scores2,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

scores3 = knn(10,train_norm @ ldac,image_label,test_norm @ ldac)
cft = confusion_matrix(np.argmax(scores3,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

m1,m2,m3 = ROC(normalise_scores(scores1),dev_label),ROC(normalise_scores(scores2),dev_label),ROC(normalise_scores(scores3),dev_label)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='Normal')
plt.plot(m2[0],m2[1],color='g',label='PCA')
plt.plot(m3[0],m3[1],color='b',label='LDA')
#plt.plot(m4[0],m4[1],color='y',label='k=150')
#plt.plot(m4[0],m4[1],color='k',label='k=200')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")


fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='Normal').plot(ax)
DetCurveDisplay(fpr=m2[0],fnr=m2[2],estimator_name='PCA').plot(ax)
DetCurveDisplay(fpr=m3[0],fnr=m3[2],estimator_name='LDA').plot(ax)
#DetCurveDisplay(fpr=m4[0],fnr=m4[2],estimator_name='model-4').plot(ax)
#DetCurveDisplay(fpr=m5[0],fnr=m5[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")

#plot ROC for PCA, LDA and normal(k=10) and det if possible



"""#SVM"""

order = np.random.permutation(len(images))
images_r = train_norm[order]
image_label_r = image_label[order]

#create a classifier
cls_img = svm.SVC(gamma=0.001, kernel="rbf", probability = True)
#train the model
cls_img.fit(images_r,image_label_r)
#predict the response
pred_img1 = cls_img.predict_proba(test_norm)

cft = confusion_matrix(np.argmax(pred_img1, axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

#PCA for 50 components
pcc = PC[1]
#create a classifier
cls_img = svm.SVC(gamma=0.001, kernel="rbf", probability = True)
#train the model
cls_img.fit(images_r @ pcc,image_label_r)
#predict the response
pred_img2 = cls_img.predict_proba(test_norm @ pcc)

cft = confusion_matrix(np.argmax(pred_img2, axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

#LDA 
#create a classifier
cls_img = svm.SVC(gamma=0.001, kernel="rbf", probability = True)
#train the model
cls_img.fit(images_r @ ldac,image_label_r)
#predict the response
pred_img3 = cls_img.predict_proba(test_norm @ ldac)

cft = confusion_matrix(np.argmax(pred_img3, axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

# #LDA after PCA
# #create a classifier
# ldac1 = LDA(train_norm @ PC[1],image_label,4)
# cls_img = svm.SVC(gamma=0.001, kernel="rbf", probability = True)
# #train the model
# cls_img.fit(train_norm @ PC[1] @ ldac1,image_label_r)
# #predict the response
# pred_img3 = cls_img.predict_proba(test_norm @ PC[1] @ ldac1)

# cft = confusion_matrix(np.argmax(pred_img3, axis=1),dev_label)
# print(np.trace(cft)/np.sum(cft))
# print(cft)



# plot roc for the above 3

scores = pred_img1

scores2 = pred_img2
scores3 = pred_img3
#scores4 = y_preddig4

m1,m2,m3 = ROC(normalise_scores(scores),dev_label),ROC(normalise_scores(scores2),dev_label),ROC(normalise_scores(scores3),dev_label)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='Normal')
plt.plot(m2[0],m2[1],color='g',label='PCA')
plt.plot(m3[0],m3[1],color='b',label='LDA')
#plt.plot(m4[0],m4[1],color='y',label='k=150')
#plt.plot(m4[0],m4[1],color='k',label='k=200')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")


fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='Normal').plot(ax)
DetCurveDisplay(fpr=m2[0],fnr=m2[2],estimator_name='PCA').plot(ax)
DetCurveDisplay(fpr=m3[0],fnr=m3[2],estimator_name='LDA').plot(ax)
#DetCurveDisplay(fpr=m4[0],fnr=m4[2],estimator_name='model-4').plot(ax)
#DetCurveDisplay(fpr=m5[0],fnr=m5[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")









"""#ANN"""

order = np.random.permutation(len(train_norm))
images_r = train_norm[order]
image_label_r = image_label[order]

label_encoder1 = LabelEncoder()
integer_encoded1 = label_encoder1.fit_transform(image_label_r)

onehot_encoder1 = OneHotEncoder(sparse=False)
integer_encoded1 = integer_encoded1.reshape(len(integer_encoded1), 1)
onehot_encoded1 = onehot_encoder1.fit_transform(integer_encoded1)

classifier2 = Sequential()
classifier2.add(Dense(units = 512, activation = 'relu', input_dim = 828))
classifier2.add(Dense(units = 128, activation = 'relu'))
classifier2.add(Dense(units = 64, activation = 'relu'))
classifier2.add(Dense(units = 5, activation = 'softmax'))

classifier2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

classifier2.fit(images_r, onehot_encoded1, batch_size = 10, epochs = 100)

y_pred = classifier2.predict(test_norm)
cft = confusion_matrix(np.argmax(y_pred,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

classifier2 = Sequential()
classifier2.add(Dense(units = 512, activation = 'relu', input_dim = 50))
classifier2.add(Dense(units = 128, activation = 'relu'))
classifier2.add(Dense(units = 64, activation = 'relu'))
classifier2.add(Dense(units = 5, activation = 'softmax'))

classifier2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier2.fit(images_r @ PC[1], onehot_encoded1, batch_size = 10, epochs = 100)

y_pred2 = classifier2.predict(test_norm @ PC[1])
cft = confusion_matrix(np.argmax(y_pred2,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)



classifier2 = Sequential()
classifier2.add(Dense(units = 32, activation = 'relu', input_dim = 4))
classifier2.add(Dense(units = 16, activation = 'relu'))
classifier2.add(Dense(units = 5, activation = 'softmax'))

classifier2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier2.fit(images_r @ ldac, onehot_encoded1, batch_size = 10, epochs = 100)

y_pred3 = classifier2.predict(test_norm @ ldac)
cft = confusion_matrix(np.argmax(y_pred3,axis=1),dev_label)
print(np.trace(cft)/np.sum(cft))
print(cft)

# plot roc for the above 3

scores = y_pred

scores2 = y_pred2
scores3 = y_pred3
m1,m2,m3 = ROC(scores,dev_label),ROC(scores2,dev_label),ROC(scores3,dev_label)
plt.figure(figsize=(9,6))
plt.plot(m1[0],m1[1],color='r',label='Normal')
plt.plot(m2[0],m2[1],color='g',label='PCA')
plt.plot(m3[0],m3[1],color='b',label='LDA')
#plt.plot(m4[0],m4[1],color='y',label='k=150')
#plt.plot(m4[0],m4[1],color='k',label='k=200')
plt.legend()
plt.grid()
plt.ylabel('TPR')
plt.xlabel('FPR')
plt.title("ROC-CURVE")


fig = plt.figure(figsize=(9,6))
ax= fig.gca()
DetCurveDisplay(fpr=m1[0],fnr=m1[2],estimator_name='Normal').plot(ax)
DetCurveDisplay(fpr=m2[0],fnr=m2[2],estimator_name='PCA').plot(ax)
DetCurveDisplay(fpr=m3[0],fnr=m3[2],estimator_name='LDA').plot(ax)
#DetCurveDisplay(fpr=m4[0],fnr=m4[2],estimator_name='model-4').plot(ax)
#DetCurveDisplay(fpr=m5[0],fnr=m5[2],estimator_name='model-4').plot(ax)
plt.title("DET-CURVE")



plt.show()
